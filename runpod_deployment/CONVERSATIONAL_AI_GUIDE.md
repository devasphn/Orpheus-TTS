# Orpheus Conversational AI Pipeline - Complete Guide

## üéØ Overview

This system integrates **Gemma 2 2B** (conversational AI) with **Orpheus TTS** (text-to-speech) to create a complete voice assistant pipeline that:

1. **Receives** user text input
2. **Generates** conversational response using Gemma 2 2B LLM
3. **Converts** response to speech using Orpheus TTS
4. **Streams** audio back to user in real-time

---

## üîç Model Selection & License Verification

### **Selected LLM: Gemma 2 2B Instruct**

#### **License Information** ‚úÖ

**License**: [Gemma Terms of Use](https://ai.google.dev/gemma/terms)

**Commercial Use**: **FULLY ALLOWED**

**Key Terms**:
- ‚úÖ Commercial use permitted without restrictions
- ‚úÖ Modification and distribution allowed
- ‚úÖ Can be used in hosted services (APIs)
- ‚úÖ Attribution required (standard practice)
- ‚ö†Ô∏è Must comply with [Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy)

**Prohibited Use Policy** (Summary):
- No illegal activities
- No harmful content generation
- No privacy violations
- No deceptive practices

**Verdict**: ‚úÖ **Meets commercial use requirements** - Permissive license with reasonable responsible AI restrictions

---

#### **Why Gemma 2 2B Was Selected**

**1. License Compatibility** ‚úÖ
- Commercial use allowed (vs. Qwen 2.5 3B which is non-commercial)
- More permissive than Llama 3.2 Community License
- Attribution acceptable per requirements

**2. Performance** üöÄ
- **Outperforms GPT-3.5** on Chatbot Arena
- **Exceptional conversational abilities** for 2B size
- **State-of-the-art** for small language models

**3. Resource Efficiency** üíæ
```
RTX A4500 VRAM: 20GB total

Model allocation:
- Orpheus TTS (3B):  ~6-7GB
- Gemma 2 2B (FP16): ~4-5GB
- SNAC decoder:      ~1GB (shared compute)
- Total:             ~11-13GB
- Headroom:          ~7-9GB ‚úÖ
```

**4. Streaming Support** ‚ö°
- Fully supported by vLLM
- Token-by-token generation
- Low latency: 50-100ms to first token

**5. Integration** üîß
- Same vLLM engine as Orpheus TTS
- Efficient GPU sharing
- Proven NVIDIA GPU performance

---

### **Alternative Models Considered**

| Model | Size | License | VRAM | Verdict |
|-------|------|---------|------|---------|
| **Gemma 2 2B** | 2B | Gemma ToU (commercial OK) | ~4GB | ‚úÖ **SELECTED** |
| Qwen 2.5 3B | 3B | Non-commercial | ~6GB | ‚ùå License incompatible |
| Qwen 2.5 7B | 7B | Apache 2.0 | ~14GB | ‚ùå Too large (21GB total) |
| Mistral 7B | 7B | Apache 2.0 | ~14GB | ‚ùå Too large (21GB total) |
| Llama 3.2 3B | 3B | Llama Community | ~6GB | ‚ö†Ô∏è Worse license + performance |

---

## üèóÔ∏è Architecture

### **Pipeline Flow**

```
User Input (Text)
    ‚Üì
[Gemma 2 2B LLM]
‚îú‚îÄ Streaming text generation
‚îú‚îÄ Sentence boundary detection
‚îî‚îÄ Text chunks
    ‚Üì
[Orpheus TTS]
‚îú‚îÄ Text ‚Üí Audio tokens
‚îú‚îÄ SNAC decoder
‚îî‚îÄ Audio chunks
    ‚Üì
User Output (Streaming Audio)
```

### **Component Breakdown**

#### **1. LLM Layer (Gemma 2 2B)**
- **Purpose**: Generate conversational responses
- **Input**: User message + conversation history
- **Output**: Streaming text tokens
- **Latency**: 50-100ms to first token
- **Throughput**: 200-300 tokens/s

#### **2. Sentence Boundary Detection**
- **Purpose**: Split LLM output into complete sentences
- **Method**: Regex-based with abbreviation handling
- **Buffering**: Accumulates text until sentence boundary
- **Output**: Complete sentences for TTS

#### **3. TTS Layer (Orpheus TTS)**
- **Purpose**: Convert text to speech
- **Input**: Complete sentences
- **Output**: Streaming audio chunks
- **Latency**: ~200ms to first audio chunk
- **Throughput**: 42 tokens/s (SNAC-limited)

#### **4. SNAC Decoder**
- **Purpose**: Convert audio tokens to waveform
- **Bottleneck**: 80% of TTS time
- **Output**: 24kHz 16-bit PCM audio

---

## üìä Performance Characteristics

### **Latency Breakdown**

```
User: "Hello, how are you?"
    ‚Üì
[LLM Generation]
‚îú‚îÄ Time to first token:     50-100ms
‚îú‚îÄ Generate response:       ~100 tokens @ 250 tok/s = 400ms
‚îî‚îÄ Total LLM time:          450-500ms

[Sentence Detection]
‚îú‚îÄ Buffer accumulation:     ~50ms
‚îî‚îÄ Sentence split:          ~10ms

[TTS Generation]
‚îú‚îÄ First sentence:          "I'm doing well, thank you!"
‚îú‚îÄ TTS processing:          ~50 tokens @ 42 tok/s = 1.2s
‚îî‚îÄ Total TTS time:          ~1.2s

[Total Pipeline]
‚îú‚îÄ Time to first audio:     ~650-750ms ‚úÖ
‚îî‚îÄ Total response time:     ~2-3s (typical conversation)
```

### **Throughput**

| Component | Throughput | Status |
|-----------|-----------|--------|
| Gemma 2 2B | 200-300 tok/s | ‚úÖ Fast |
| Sentence Detection | ~1000 tok/s | ‚úÖ Fast |
| Orpheus TTS | 42 tok/s | ‚ö†Ô∏è Bottleneck |
| **Pipeline** | **42 tok/s** | **Limited by TTS** |

**Note**: The TTS layer (42 tok/s) is the bottleneck, not the LLM. This is expected and acceptable for high-quality speech synthesis.

---

## üöÄ API Usage

### **Endpoint: `/chat`**

**Method**: POST

**Request Body**:
```json
{
  "message": "Hello, how are you?",
  "voice": "tara",
  "history": [
    {"role": "user", "content": "Hi there!"},
    {"role": "assistant", "content": "Hello! How can I help you today?"}
  ],
  "stream_text": false
}
```

**Parameters**:
- `message` (required): User's message
- `voice` (optional): Voice name (default: "tara")
  - Options: tara, zoe, zac, jess, leo, mia, julia, leah
- `history` (optional): Conversation history (list of {role, content})
- `stream_text` (optional): If true, returns text stream instead of audio (for debugging)

**Response**:
- **Content-Type**: `audio/wav`
- **Format**: Streaming WAV audio (24kHz, 16-bit, mono)

**Example (cURL)**:
```bash
curl -X POST http://localhost:8080/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Tell me a joke",
    "voice": "zoe"
  }' \
  --output response.wav
```

**Example (Python)**:
```python
import requests

response = requests.post(
    "http://localhost:8080/chat",
    json={
        "message": "What's the weather like?",
        "voice": "tara",
        "history": []
    },
    stream=True
)

with open("response.wav", "wb") as f:
    for chunk in response.iter_content(chunk_size=8192):
        if chunk:
            f.write(chunk)
```

**Example (JavaScript)**:
```javascript
fetch('http://localhost:8080/chat', {
  method: 'POST',
  headers: {'Content-Type': 'application/json'},
  body: JSON.stringify({
    message: 'Hello!',
    voice: 'tara'
  })
})
.then(response => response.blob())
.then(blob => {
  const audio = new Audio(URL.createObjectURL(blob));
  audio.play();
});
```

---

### **Endpoint: `/tts`** (Original TTS-only)

**Method**: GET or POST

**Parameters**:
- `prompt`: Text to convert to speech
- `voice`: Voice name (default: "tara")
- `temperature`: Generation temperature (default: 0.4)
- `top_p`: Nucleus sampling (default: 0.9)
- `max_tokens`: Maximum tokens (default: 2000)
- `repetition_penalty`: Repetition penalty (default: 1.1)

**Example**:
```bash
curl "http://localhost:8080/tts?prompt=Hello%20world&voice=zoe" --output hello.wav
```

---

### **Endpoint: `/models`**

**Method**: GET

**Response**:
```json
{
  "llm": {
    "name": "Gemma 2 2B Instruct",
    "provider": "Google",
    "parameters": "2B",
    "license": "Gemma Terms of Use (Commercial use allowed)",
    "license_url": "https://ai.google.dev/gemma/terms",
    "capabilities": ["conversational_ai", "streaming", "instruction_following"],
    "loaded": true
  },
  "tts": {
    "name": "Orpheus TTS",
    "provider": "Canopy Labs",
    "parameters": "3B",
    "license": "Apache 2.0",
    "capabilities": ["text_to_speech", "voice_cloning", "streaming", "emotion_control"],
    "voices": ["tara", "zoe", "zac", "jess", "leo", "mia", "julia", "leah"],
    "loaded": true
  }
}
```

---

## ‚öôÔ∏è Configuration

### **Environment Variables**

```bash
# LLM Configuration
export LLM_MODEL_NAME="google/gemma-2-2b-it"
export LLM_MAX_MODEL_LEN="2048"
export LLM_GPU_MEMORY_UTILIZATION="0.25"  # 25% of GPU VRAM

# TTS Configuration
export TTS_MODEL_NAME="canopylabs/orpheus-tts-0.1-finetune-prod"
export TTS_MAX_MODEL_LEN="2048"
export TTS_GPU_MEMORY_UTILIZATION="0.35"  # 35% of GPU VRAM

# Server Configuration
export PORT="8080"
export SNAC_DEVICE="cuda"
```

### **GPU Memory Allocation Strategy**

```
RTX A4500: 20GB VRAM

Allocation:
- TTS (35%):     ~7GB  (Orpheus TTS 3B)
- LLM (25%):     ~5GB  (Gemma 2 2B)
- Total:         60%   (~12GB)
- Headroom:      40%   (~8GB for SNAC, overhead, KV cache)
```

**Why this allocation?**
- TTS needs more memory (larger model + KV cache for long audio)
- LLM is smaller and faster (2B vs 3B)
- 40% headroom ensures SNAC decoder has GPU compute available
- Prevents OOM errors during concurrent requests

---

## üîß Installation & Deployment

### **Prerequisites**

```bash
# System requirements
- NVIDIA GPU with 20GB+ VRAM (RTX A4500, A5000, A6000, etc.)
- CUDA 11.7+
- Python 3.10+
- 50GB+ disk space
```

### **Installation Steps**

```bash
# 1. Clone repository
cd /workspace/Orpheus-TTS

# 2. Install Orpheus TTS from local repository
cd orpheus_tts_pypi
pip install -e .
cd ..

# 3. Install vLLM and dependencies
pip install vllm==0.7.3
pip install flask requests

# 4. Set environment variables
export SNAC_DEVICE=cuda
export LLM_GPU_MEMORY_UTILIZATION=0.25
export TTS_GPU_MEMORY_UTILIZATION=0.35

# 5. Start server
cd runpod_deployment
python server.py
```

### **Verification**

```bash
# Check health
curl http://localhost:8080/health

# Expected response:
{
  "status": "healthy",
  "models": {
    "tts": true,
    "llm": true
  },
  "features": {
    "text_to_speech": true,
    "conversational_ai": true,
    "streaming": true
  }
}

# Test conversational AI
curl -X POST http://localhost:8080/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello!"}' \
  --output test.wav

# Play audio
# (On Linux) aplay test.wav
# (On Mac) afplay test.wav
# (On Windows) start test.wav
```

---

## üìà Performance Benchmarks

### **Expected Performance (RTX A4500)**

| Metric | Value | Notes |
|--------|-------|-------|
| Time to first audio | 650-750ms | LLM + sentence detection + TTS startup |
| LLM throughput | 200-300 tok/s | Gemma 2 2B generation speed |
| TTS throughput | 42 tok/s | Limited by SNAC decoder |
| Pipeline throughput | 42 tok/s | Bottlenecked by TTS |
| Typical response time | 2-3s | For short conversational responses |
| Long response time | 5-10s | For paragraph-length responses |
| GPU memory usage | 12-13GB | 60% of 20GB VRAM |
| Concurrent requests | 1-2 | Limited by GPU memory |

### **Comparison to Alternatives**

| System | Latency | Quality | License | VRAM |
|--------|---------|---------|---------|------|
| **Orpheus + Gemma 2 2B** | ~700ms | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Commercial | 12GB |
| ElevenLabs API | ~500ms | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üí∞ Paid | N/A |
| OpenAI TTS | ~300ms | ‚≠ê‚≠ê‚≠ê‚≠ê | üí∞ Paid | N/A |
| Coqui TTS + GPT-2 | ~1000ms | ‚≠ê‚≠ê‚≠ê | ‚úÖ Open | 8GB |

---

## üêõ Troubleshooting

### **Issue: Models fail to load**

**Symptoms**:
```
Failed to load models: CUDA out of memory
```

**Solution**:
```bash
# Reduce GPU memory allocation
export LLM_GPU_MEMORY_UTILIZATION=0.20
export TTS_GPU_MEMORY_UTILIZATION=0.30

# Or use smaller LLM max_model_len
export LLM_MAX_MODEL_LEN=1024
```

### **Issue: Slow response time**

**Symptoms**:
- Time to first audio > 2 seconds
- Total response time > 10 seconds

**Diagnosis**:
```bash
# Check server logs for timing information
# Look for:
# - [SNAC Profile] messages (should be ~135ms per chunk)
# - vLLM throughput stats (should be 200+ tok/s for LLM)
```

**Solution**:
- This is expected behavior (TTS is the bottleneck at 42 tok/s)
- For faster responses, use shorter prompts or reduce max_tokens
- Consider using `stream_text=true` to test LLM speed independently

### **Issue: Audio cuts off mid-sentence**

**Symptoms**:
- Generated audio is incomplete
- Response ends abruptly

**Solution**:
```bash
# Increase TTS max_tokens
export TTS_MAX_MODEL_LEN=2048  # Keep at 2048 for full audio

# Or reduce LLM max_tokens to generate shorter responses
# In API request:
{
  "message": "...",
  "max_tokens": 256  # Shorter LLM responses
}
```

---

## üìö Additional Resources

- **Gemma 2 Documentation**: https://ai.google.dev/gemma/docs
- **Gemma License**: https://ai.google.dev/gemma/terms
- **Orpheus TTS GitHub**: https://github.com/canopyai/Orpheus-TTS
- **vLLM Documentation**: https://docs.vllm.ai/

---

## ‚úÖ Summary

**What You Have**:
- ‚úÖ Conversational AI with Gemma 2 2B (commercial license)
- ‚úÖ High-quality TTS with Orpheus (8 voices)
- ‚úÖ Streaming audio responses
- ‚úÖ ~700ms latency to first audio
- ‚úÖ Production-ready API

**Performance**:
- ‚úÖ 42 tokens/s throughput (TTS-limited)
- ‚úÖ 200-300 tokens/s LLM generation
- ‚úÖ Near real-time for short responses

**Next Steps**:
1. Test the `/chat` endpoint
2. Integrate with your application
3. Monitor performance and adjust parameters
4. Consider adding conversation history management
5. Implement caching for common responses

**License Compliance**:
- ‚úÖ Gemma 2 2B: Commercial use allowed (Gemma Terms of Use)
- ‚úÖ Orpheus TTS: Apache 2.0 (fully permissive)
- ‚úÖ Attribution provided in `/models` endpoint

Enjoy your conversational AI voice assistant! üéâ

