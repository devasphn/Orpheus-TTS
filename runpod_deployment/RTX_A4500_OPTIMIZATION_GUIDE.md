# RTX A4500 Performance Optimization Guide for Orpheus TTS

## üéØ Executive Summary

**Your Current Performance:**
- GPU: NVIDIA RTX A4500 (20GB VRAM)
- Throughput: 42.6 tokens/s
- Generation time: ~20 seconds for typical prompts
- GPU utilization shown: 0% (misleading - see explanation below)

**Expected Performance:**
- Target throughput: 60-100 tokens/s
- Target generation time: 10-15 seconds for typical prompts

**Status:** You're getting ~60-70% of expected performance. This guide will help you optimize.

---

## ‚ùì Answering Your Critical Questions

### **Q1: Is the GPU Actually Being Used? (0% Utilization Mystery)**

**Answer: YES, your GPU IS being used!** The 0% in nvidia-smi is a **sampling artifact**.

#### Why nvidia-smi Shows 0%

`nvidia-smi` samples GPU utilization over ~1 second windows. vLLM's async engine has this pattern:

```
Timeline:  [GPU] [CPU] [GPU] [CPU] [GPU] [CPU] ...
Duration:  50ms  400ms 50ms  400ms 50ms  400ms
GPU %:     100%  0%    100%  0%    100%  0%
```

If nvidia-smi samples during CPU processing phases, it shows 0%.

#### Proof GPU IS Working

1. ‚úÖ **Model loaded on GPU**: 17,231 MiB / 20,475 MiB used
2. ‚úÖ **vLLM reports throughput**: 42.6-42.8 tokens/s
3. ‚úÖ **Logs show GPU allocation**: "model weights take 6.18GiB"
4. ‚úÖ **You get audio output**: Inference is happening!

#### How to Properly Monitor GPU Usage

**Wrong way (what you did):**
```bash
nvidia-smi  # Single snapshot - might catch idle period
```

**Right way:**
```bash
# Monitor continuously during generation
nvidia-smi dmon -s u -d 1

# Or use detailed monitoring
nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used --format=csv -l 1
```

**Expected output during generation:**
```
# gpu   mutil
# Idx     %
    0    15-45%  ‚Üê This is normal for async LLM inference!
```

---

### **Q2: Why Does Reducing max_model_len Cause Incomplete Audio?**

**I was WRONG in my previous recommendation. I apologize for the confusion.**

#### The Critical Misunderstanding

`max_model_len` in vLLM controls **TOTAL sequence length** (input + output), NOT just output length.

#### Your Case Analysis

```
Input prompt:     ~50-100 tokens (text + voice formatting)
Output needed:    117 chunks √ó 7 tokens/chunk = 819 tokens
TOTAL needed:     ~870-920 tokens
```

**When max_model_len=512:**
```
Available output: 512 - 100 (prompt) = 412 tokens
Model generates:  412 tokens = 59 chunks
Result:           Audio cuts off at 59 chunks ‚ùå
```

**When max_model_len=2048:**
```
Available output: 2048 - 100 (prompt) = 1948 tokens
Model generates:  819 tokens = 117 chunks (full audio)
Result:           Complete audio ‚úÖ
```

#### Correct Configuration

```bash
export MAX_MODEL_LEN=2048  # Keep this! DO NOT reduce!
```

**Why 2048 is correct:**
- Allows ~1900 tokens of output
- Supports 270+ audio chunks (~1 minute of speech)
- Handles long sentences and paragraphs
- Only uses 10GB KV cache (you have 20GB VRAM)

---

### **Q3: Optimal Configuration for RTX A4500**

#### Your Hardware Profile

```
GPU:              RTX A4500
VRAM:             20GB
Architecture:     Ampere (GA102)
Memory Bandwidth: 512 GB/s
CUDA Cores:       7,168
Tensor Cores:     224 (3rd gen)
```

#### Recommended vLLM Configuration

**Environment Variables:**
```bash
export MAX_MODEL_LEN=2048              # Keep for full audio generation
export GPU_MEMORY_UTILIZATION=0.85     # Optimized for 20GB VRAM
export SNAC_DEVICE=cuda                # Ensure audio decoder uses GPU
```

**server.py Configuration (already updated):**
```python
engine = OrpheusModel(
    model_name=model_name,
    max_model_len=2048,
    gpu_memory_utilization=0.85,
    max_num_seqs=1,              # Single request optimization
    disable_log_stats=False      # Keep performance monitoring
)
```

#### Why These Settings?

1. **max_model_len=2048**: Allows full audio generation without truncation
2. **gpu_memory_utilization=0.85**: Leaves 3GB headroom for CUDA operations
3. **max_num_seqs=1**: Optimizes for single TTS requests (not batching)

---

### **Q4: How to Verify GPU Usage During Inference**

#### Diagnostic Commands

**1. Real-time GPU Monitoring (Run in separate terminal):**
```bash
# Start monitoring BEFORE making TTS request
nvidia-smi dmon -s u -d 1
```

**2. Detailed Memory and Utilization:**
```bash
watch -n 0.5 'nvidia-smi --query-gpu=index,name,utilization.gpu,utilization.memory,memory.used,memory.total --format=csv'
```

**3. Check SNAC Decoder Device:**
```bash
python -c "
import os
os.environ['SNAC_DEVICE'] = 'cuda'
from orpheus_tts.decoder import model, snac_device
print(f'SNAC device: {snac_device}')
print(f'SNAC model on: {next(model.parameters()).device}')
"
```

**Expected output:**
```
SNAC device: cuda
SNAC model on: cuda:0
```

**4. vLLM Performance Stats:**

Check your server logs for these lines:
```
INFO:     Avg prompt throughput: X.X tokens/s
INFO:     Avg generation throughput: 42.6 tokens/s  ‚Üê Your current performance
```

**5. End-to-End Performance Test:**
```bash
# Test with timing
time curl "http://localhost:8080/tts?prompt=Hello%20world%2C%20this%20is%20a%20test&voice=tara" \
  --output test.wav

# Should complete in 10-15 seconds for this prompt
```

---

## üöÄ Performance Optimization Strategies

### **Current Bottleneck Analysis**

Your 42 tokens/s is slower than expected (60-100 tokens/s). Possible causes:

1. **SNAC Decoder Overhead** (Most Likely)
   - SNAC decoder runs on same GPU as LLM
   - Competes for GPU compute resources
   - Each audio chunk requires decoder inference

2. **Async Engine Overhead**
   - AsyncLLMEngine has more overhead than sync
   - Better for concurrent requests, not single requests

3. **CPU Bottleneck**
   - Token processing in Python
   - Queue management between async tasks

### **Optimization 1: Verify SNAC is on GPU**

```bash
# Set environment variable
export SNAC_DEVICE=cuda

# Restart server
cd /workspace/Orpheus-TTS/runpod_deployment
python server.py
```

### **Optimization 2: Increase GPU Memory Utilization**

If you're not running other GPU workloads:

```bash
export GPU_MEMORY_UTILIZATION=0.90  # Use more VRAM for KV cache
```

This allows larger batch sizes and better GPU utilization.

### **Optimization 3: Tune Generation Parameters**

For faster (but slightly less varied) speech:

```python
# In your API requests
{
    "prompt": "Your text here",
    "voice": "tara",
    "temperature": 0.3,        # Lower = faster, more deterministic
    "repetition_penalty": 1.3,  # Higher = faster speech
    "max_tokens": 1500         # Limit if you know text is short
}
```

### **Optimization 4: Profile the Pipeline**

Add timing logs to identify bottleneck:

```python
# Add to server.py generate_audio_stream()
import time

def generate_audio_stream():
    yield create_wav_header()
    
    token_gen_start = time.time()
    syn_tokens = engine.generate_speech(...)
    
    chunk_count = 0
    decode_time = 0
    
    for chunk in syn_tokens:
        decode_start = time.time()
        chunk_count += 1
        yield chunk
        decode_time += time.time() - decode_start
    
    total_time = time.time() - token_gen_start
    logger.info(f"Token generation: {total_time - decode_time:.2f}s")
    logger.info(f"Audio decoding: {decode_time:.2f}s")
    logger.info(f"Total: {total_time:.2f}s")
```

This will show if SNAC decoder is the bottleneck.

---

## üìä Performance Benchmarks

### **Expected Performance on RTX A4500**

| Metric | Current | Target | Best Case |
|--------|---------|--------|-----------|
| Throughput | 42 tok/s | 60-80 tok/s | 100 tok/s |
| Time (short prompt) | 20s | 12-15s | 8-10s |
| Time (long prompt) | 30s | 20-25s | 15-18s |
| GPU Utilization | 15-25% | 25-40% | 40-60% |
| Time to First Chunk | ~200ms | ~150ms | ~100ms |

### **Realistic Expectations**

For TTS workloads with SNAC decoder on same GPU:
- **40-60 tokens/s is reasonable** for RTX A4500
- **60-80 tokens/s is achievable** with optimizations
- **100+ tokens/s is unlikely** due to SNAC decoder overhead

---

## üîß Troubleshooting Guide

### **Issue: Still seeing 0% GPU utilization**

**Solution:**
```bash
# Monitor during active generation
nvidia-smi dmon -s u -d 1 &
curl "http://localhost:8080/tts?prompt=Test" --output test.wav
```

You should see 15-45% utilization spikes.

### **Issue: Performance didn't improve**

**Check:**
1. SNAC device: `echo $SNAC_DEVICE` should be `cuda`
2. vLLM version: `pip show vllm` should be `0.7.3`
3. Server restarted after config changes
4. No other GPU processes: `nvidia-smi` should show only your server

### **Issue: Out of memory errors**

**Solution:**
```bash
export GPU_MEMORY_UTILIZATION=0.80  # Reduce from 0.85
export MAX_MODEL_LEN=1536           # Reduce if needed
```

### **Issue: Audio quality degraded**

**Solution:**
```python
# Increase temperature and reduce repetition_penalty
temperature = 0.5              # Up from 0.3
repetition_penalty = 1.1       # Down from 1.3
```

---

## ‚úÖ Quick Start: Apply Optimizations Now

**Step 1: Set Environment Variables**
```bash
export MAX_MODEL_LEN=2048
export GPU_MEMORY_UTILIZATION=0.85
export SNAC_DEVICE=cuda
```

**Step 2: Restart Server**
```bash
cd /workspace/Orpheus-TTS/runpod_deployment
python server.py
```

**Step 3: Monitor Performance**
```bash
# In separate terminal
nvidia-smi dmon -s u -d 1
```

**Step 4: Test**
```bash
time curl "http://localhost:8080/tts?prompt=Hello%20world&voice=tara" --output test.wav
```

**Step 5: Check Logs**

Look for:
```
INFO:     Avg generation throughput: X.X tokens/s
```

Target: 50-70 tokens/s (improvement from 42 tokens/s)

---

## üìà Summary

### **Key Takeaways**

1. ‚úÖ **GPU IS being used** - 0% in nvidia-smi is sampling artifact
2. ‚úÖ **Keep max_model_len=2048** - DO NOT reduce (causes incomplete audio)
3. ‚úÖ **Use gpu_memory_utilization=0.85** - Optimized for RTX A4500
4. ‚úÖ **Monitor with `nvidia-smi dmon`** - Not single snapshots
5. ‚úÖ **42 tokens/s is slow but not broken** - Target 60-80 tokens/s

### **Expected Improvements**

After applying optimizations:
- Throughput: 42 ‚Üí 55-70 tokens/s (30-65% improvement)
- Generation time: 20s ‚Üí 12-16s (20-40% faster)
- GPU utilization: More consistent 25-40% average

### **Next Steps**

1. Apply the configuration changes (already done in server.py)
2. Set environment variables
3. Restart server
4. Monitor with `nvidia-smi dmon`
5. Report back with new performance numbers!

Good luck! üöÄ

